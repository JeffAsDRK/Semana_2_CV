{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730c5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta principal donde están las carpetas como 'abyss', 'gran_paradiso_island', etc.\n",
    "base_path = 'dense_data'\n",
    "\n",
    "# Inicializamos una lista para guardar los datos\n",
    "data = []\n",
    "\n",
    "# Recorremos cada subcarpeta (nivel de mapa)\n",
    "for level_name in os.listdir(base_path):\n",
    "    level_path = os.path.join(base_path, level_name)\n",
    "\n",
    "    # Nos aseguramos de que sea una carpeta\n",
    "    if not os.path.isdir(level_path):\n",
    "        continue\n",
    "\n",
    "    # Rutas de interés: frame y combined_visual\n",
    "    frame_path = os.path.join(level_path, 'frame')\n",
    "    combined_visual_path = os.path.join(level_path, 'combined')\n",
    "\n",
    "    # Leemos los nombres de archivos de cada carpeta (si existen)\n",
    "    frame_images = os.listdir(frame_path) if os.path.isdir(frame_path) else []\n",
    "    combined_visual_images = os.listdir(combined_visual_path) if os.path.isdir(combined_visual_path) else []\n",
    "\n",
    "    # Ajustamos la cantidad para que estén pareados (opcional: puedes usar el largo máximo)\n",
    "    max_len = max(len(frame_images), len(combined_visual_images))\n",
    "\n",
    "    # Completamos con None si las listas no son del mismo tamaño\n",
    "    frame_images += [None] * (max_len - len(frame_images))\n",
    "    combined_visual_images += [None] * (max_len - len(combined_visual_images))\n",
    "\n",
    "    # Guardamos en la lista de datos\n",
    "    for f_img, cv_img in zip(frame_images, combined_visual_images):\n",
    "        data.append({\n",
    "            'name': level_name,\n",
    "            'frame': f_img,\n",
    "            'combined': cv_img\n",
    "        })\n",
    "\n",
    "# Creamos el DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ab5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransferSegmentation, save_model, load_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from utils_jeff import Load_Dataset\n",
    "import torch.utils.tensorboard as tb\n",
    "import numpy as np\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "datase = Load_Dataset(df, base_path, batch_size=64, aumentation=6, num_workers=4)\n",
=======
    "datase = Load_Dataset(df, base_path, batch_size=128, aumentation=10, num_workers=4)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "datase = Load_Dataset(df, base_path, batch_size=128, aumentation=10, num_workers=4)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "datase = Load_Dataset(df, base_path, batch_size=128, aumentation=10, num_workers=4)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "datase = Load_Dataset(df, base_path, batch_size=128, aumentation=10, num_workers=4)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "### cambiar ####\n",
    "\n",
    "\n",
    "def post_proces(predicted_logits):\n",
    "    predicted_mask = torch.argmax(predicted_logits, dim=1)  # [B, H, W]\n",
    "    return predicted_mask\n",
    "\n",
    "\n",
    "def calculate_multiclass_iou_f1(\n",
    "    predicted_logits, target_mask, num_classes, epsilon=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula el IoU y el F1-score por clase y sus promedios (mIoU, macro-F1).\n",
    "\n",
    "    Args:\n",
    "        predicted_logits: Tensor [B, C, H, W] (output sin softmax)\n",
    "        target_mask: Tensor [B, H, W] con valores enteros de clase\n",
    "        num_classes: Total de clases\n",
    "    Returns:\n",
    "        mean_iou: Promedio de IoU entre clases\n",
    "        mean_f1: Promedio de F1 entre clases\n",
    "    \"\"\"\n",
    "    predicted_mask = post_proces(predicted_logits)\n",
    "\n",
    "    ious = []\n",
    "    f1s = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = predicted_mask == cls\n",
    "        target_cls = target_mask == cls\n",
    "\n",
    "        intersection = (pred_cls & target_cls).sum().float()\n",
    "        union = (pred_cls | target_cls).sum().float()\n",
    "\n",
    "        # IoU\n",
    "        iou = (intersection + epsilon) / (union + epsilon)\n",
    "        ious.append(iou)\n",
    "\n",
    "        # F1-score\n",
    "        tp = intersection\n",
    "        fp = (pred_cls & ~target_cls).sum().float()\n",
    "        fn = (~pred_cls & target_cls).sum().float()\n",
    "        f1 = (2 * tp + epsilon) / (2 * tp + fp + fn + epsilon)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    mean_iou = torch.mean(torch.tensor(ious))\n",
    "    mean_f1 = torch.mean(torch.tensor(f1s))\n",
    "\n",
    "    return mean_iou.item(), mean_f1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "id": "c04f8eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos por clase: tensor([8.3391e-04, 9.1968e-04, 1.9312e-02, 3.5804e-01, 2.1843e+00, 1.0238e+00,\n",
      "        3.4128e+00])\n"
     ]
    }
   ],
   "source": [
    "# Número de clases\n",
    "NUM_CLASSES = 7\n",
    "# Inicializar contador de clases\n",
    "pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "import cv2\n",
    "# Recorremos todas las máscaras\n",
    "for _, row in df.iterrows():\n",
    "    mask_path = f\"./{base_path}/{row['name']}/combined/{row['combined']}\"\n",
    "    mask = cv2.imread(mask_path)\n",
    "    mask=cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)  # matriz de clases (HxW)\n",
    "    # Contar ocurrencias de cada clase\n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        if cls < NUM_CLASSES:\n",
    "            pixel_counts[cls] += cnt\n",
    "\n",
    "# Calcular pesos inversos (más peso a clases menos frecuentes)\n",
    "class_weights = 1.0 / (pixel_counts + 1e-6)  # para evitar división por cero\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # normalizar\n",
    "\n",
    "# Convertir a tensor para usar en CrossEntropyLoss\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "print(\"Pesos por clase:\", class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
   "id": "084bc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransferSegmentation(n_classes=7)\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "\n",
=======
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "#transform = v2.Compose([\n",
    "#    #v2.ToImage(),\n",
    "#    v2.Resize((224, 224))\n",
    "#])\n",
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "transform = model.weights.transforms() # resulta mejor con la transformacion propia\n",
    "\n",
    "\n",
    "train_loader = datase.load_train(transform=transform)\n",
    "val_loader = datase.load_val(transform=transform)\n",
    "test_loader = datase.load_test(transform=transform)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device)) # aplica sigmoid directamente sobre los logits del modelo\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,                # learning rate inicial\n",
    "    weight_decay=1e-4,      # regularización L2\n",
    "    betas=(0.9, 0.999),     # valores por defecto para Adam\n",
    "    eps=1e-8                # estabilidad numérica\n",
    ")\n",
=======
    "criterion = nn.CrossEntropyLoss() # aplica sigmoid directamente sobre los logits del modelo\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "criterion = nn.CrossEntropyLoss() # aplica sigmoid directamente sobre los logits del modelo\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "criterion = nn.CrossEntropyLoss() # aplica sigmoid directamente sobre los logits del modelo\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Trainer:\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "\n",
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        log_dir=None,\n",
    "        num_classes=7,\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "        max_epochs=200,\n",
    "        max_patience=25,\n",
=======
    "        max_epochs=100,\n",
    "        max_patience=15,\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "        max_epochs=100,\n",
    "        max_patience=15,\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "        max_epochs=100,\n",
    "        max_patience=15,\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.max_epochs = max_epochs\n",
    "        self.max_patience = max_patience\n",
    "\n",
    "        self.best_iou = 0\n",
    "        self.patience = 0\n",
    "        self.global_step = 0\n",
    "        self.writer = SummaryWriter(log_dir, flush_secs=1) if log_dir else None\n",
    "        self.val_stoping=0\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "        self.train_stoping=0\n",
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "\n",
    "    def train(self):\n",
    "        print(f\"{'Epoch':<7}{'Phase':<8}{'Loss':<10}{'mIoU (%)':<10}{'F1 (%)':<10}\")\n",
    "        print(\"=\" * 45)\n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss, epoch_iou, epoch_f1 = self.train_one_epoch(epoch)\n",
    "    \n",
    "            self.scheduler.step(epoch_iou)\n",
    "    \n",
    "            # Evaluación después de entrenamiento\n",
    "            val_iou, val_f1 = self.evaluate(epoch)\n",
    "    \n",
    "            # Mostrar resumen en columna\n",
    "            print(f\"{epoch+1:<7}{'Train':<8}{epoch_loss:<10.4f}{epoch_iou:<10.2f}{epoch_f1:<10.2f}\")\n",
    "            print(f\"{'':<7}{'Val':<8}{'-':<10}{val_iou:<10.2f}{val_f1:<10.2f}\")\n",
    "    \n",
    "            # TensorBoard logging\n",
    "            if self.writer:\n",
    "                self.writer.add_scalar(\"epoch_loss\", epoch_loss, epoch)\n",
    "                self.writer.add_scalar(\"epoch_mean_iou\", epoch_iou, epoch)\n",
    "                self.writer.add_scalar(\"epoch_mean_f1\", epoch_f1, epoch)\n",
    "                self.writer.add_scalar(\"val_mean_iou\", val_iou, epoch)\n",
    "                self.writer.add_scalar(\"val_mean_f1\", val_f1, epoch)\n",
    "                self.writer.add_scalar(\"epoch_lr\", self.optimizer.param_groups[0][\"lr\"], epoch)\n",
    "    \n",
    "            # Guardado del mejor modelo\n",
    "            \n",
    "            if val_iou > self.best_iou:\n",
    "                self.save_model(\"TransferSegmentation_Jeff.th\")\n",
    "                self.best_iou = val_iou\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "                self.patience = 0\n",
    "\n",
    "            alpha = 0.9\n",
    "            penalized = val_iou - alpha * abs(val_iou - epoch_iou) \n",
    "            \n",
    "            if penalized > self.val_stoping:\n",
    "                self.val_stoping = penalized\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "                self.patience = 0\n",
    "\n",
    "            if val_iou < self.val_stoping:\n",
    "               if epoch_iou > self.train_stoping:\n",
    "                   self.train_stoping=epoch_iou\n",
    "                   self.patience=0\n",
    "               else:\n",
    "                   self.patience+=1 \n",
    "            else: \n",
    "                self.val_stoping=val_iou\n",
    "                self.patience=0\n",
=======
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
    "                self.patience = 0\n",
    "            else:\n",
    "                self.patience += 1\n",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
    "    \n",
    "            if self.patience >= self.max_patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0\n",
    "        ious, f1 = [], []\n",
    "        total = len(self.train_loader)\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(self.train_loader):\n",
    "            print(f\"Train Epoch {epoch+1}  {(i+1)*100/total:.2f}%\", end=\"\\r\")\n",
    "            inputs = inputs.to(self.device,non_blocking=True)\n",
    "            targets = targets.long().squeeze().to(self.device,non_blocking=True)\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            batch_iou, f1_score = calculate_multiclass_iou_f1(\n",
    "                outputs, targets, self.num_classes\n",
    "            )\n",
    "            ious.append(batch_iou)\n",
    "            f1.append(f1_score)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        mean_iou = np.mean(ious) * 100\n",
    "        mean_f1 = np.mean(f1) * 100\n",
    "        avg_loss = running_loss / total\n",
    "        return avg_loss, mean_iou, mean_f1\n",
    "\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        ious, f1 = [], []\n",
    "        total = len(self.val_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, targets) in enumerate(self.val_loader):\n",
    "                print(f\"Val   Epoch {epoch+1}  {(i+1)*100/total:.2f}%\", end=\"\\r\")\n",
    "                inputs = inputs.to(self.device,non_blocking=True)\n",
    "                targets = targets.long().squeeze().to(self.device,non_blocking=True)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                batch_iou, f1_score = calculate_multiclass_iou_f1(\n",
    "                    outputs, targets, self.num_classes\n",
    "                )\n",
    "                ious.append(batch_iou)\n",
    "                f1.append(f1_score)\n",
    "            print()\n",
    "\n",
    "        mean_iou = np.mean(ious) * 100\n",
    "        mean_f1 = np.mean(f1) * 100\n",
    "\n",
    "        if self.writer:\n",
    "            self.writer.add_scalar(\"val_mean_iou\", mean_iou, epoch)\n",
    "            self.writer.add_scalar(\"val_mean_f1\", mean_f1, epoch)\n",
    "            vis_targets = targets.unsqueeze(1) * 32 / 255\n",
    "            pred_mask = post_proces(outputs)\n",
    "            vis_outputs = pred_mask.unsqueeze(1) * 32 / 255\n",
    "            self.writer.add_images(\"val/image\", inputs[:6].cpu(), epoch)\n",
    "            self.writer.add_images(\"val/label\", vis_targets[:6].cpu(), epoch)\n",
    "            self.writer.add_images(\"val/pred\", vis_outputs[:6].cpu(), epoch)\n",
    "\n",
    "            if self.global_step == 0:\n",
    "                self.writer.add_graph(self.model, inputs)\n",
    "        self.global_step += 1\n",
    "        return mean_iou, mean_f1\n",
    "\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "        print(f\"Modelo guardado como {filename}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "execution_count": null,
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "execution_count": null,
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "execution_count": null,
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
   "id": "a2c755f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  Phase   Loss      mIoU (%)  F1 (%)    \n",
      "=============================================\n",
      "Val   Epoch 1  100.00%\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "1      Train   0.9999    25.41     32.65     \n",
      "       Val     -         19.02     26.57     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 2  100.00%\n",
      "2      Train   0.6147    25.79     33.32     \n",
      "       Val     -         23.85     30.91     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 3  100.00%\n",
      "3      Train   0.6558    23.26     30.82     \n",
      "       Val     -         19.16     26.11     \n",
      "Val   Epoch 4  100.00%\n",
      "4      Train   0.5984    23.98     31.60     \n",
      "       Val     -         23.78     31.08     \n",
      "Val   Epoch 5  100.00%\n",
      "5      Train   0.4993    25.92     33.44     \n",
      "       Val     -         21.63     28.61     \n",
      "Val   Epoch 6  100.00%\n",
      "6      Train   0.4453    27.13     34.60     \n",
      "       Val     -         29.06     35.64     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 7  100.00%\n",
      "7      Train   0.4319    28.12     35.37     \n",
      "       Val     -         27.69     34.78     \n",
      "Val   Epoch 8  100.00%\n",
      "8      Train   0.3617    29.65     36.78     \n",
      "       Val     -         27.54     34.64     \n",
      "Val   Epoch 9  100.00%\n",
      "9      Train   0.3187    31.18     38.33     \n",
      "       Val     -         30.41     37.72     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 10  100.00%\n",
      "10     Train   0.3497    30.45     37.92     \n",
      "       Val     -         28.77     35.64     \n",
      "Val   Epoch 11  100.00%\n",
      "11     Train   0.3308    31.89     39.06     \n",
      "       Val     -         32.04     39.02     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 12  100.00%\n",
      "12     Train   0.2860    33.77     40.96     \n",
      "       Val     -         25.04     32.11     \n",
      "Val   Epoch 13  100.00%\n",
      "13     Train   0.3546    31.23     38.36     \n",
      "       Val     -         27.56     34.16     \n",
      "Val   Epoch 14  100.00%\n",
      "14     Train   0.2972    32.29     39.50     \n",
      "       Val     -         32.63     39.33     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 15  100.00%\n",
      "15     Train   0.2640    33.87     40.74     \n",
      "       Val     -         35.96     43.93     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 16  100.00%\n",
      "16     Train   0.2490    35.92     43.02     \n",
      "       Val     -         30.21     37.83     \n",
      "Val   Epoch 17  100.00%\n",
      "17     Train   0.4512    27.49     35.08     \n",
      "       Val     -         27.76     35.35     \n",
      "Val   Epoch 18  100.00%\n",
      "18     Train   0.3373    30.73     38.08     \n",
      "       Val     -         30.69     38.05     \n",
      "Val   Epoch 19  100.00%\n",
      "19     Train   0.3569    30.58     38.04     \n",
      "       Val     -         27.45     34.70     \n",
      "Val   Epoch 20  100.00%\n",
      "20     Train   0.3194    32.54     39.94     \n",
      "       Val     -         21.69     28.83     \n",
      "Val   Epoch 21  100.00%\n",
      "21     Train   0.3796    29.19     36.27     \n",
      "       Val     -         28.82     36.65     \n",
      "Val   Epoch 22  100.00%\n",
      "22     Train   0.2956    32.49     39.63     \n",
      "       Val     -         33.30     40.89     \n",
      "Val   Epoch 23  100.00%\n",
      "23     Train   0.2846    33.62     41.04     \n",
      "       Val     -         31.61     38.90     \n",
      "Val   Epoch 24  100.00%\n",
      "24     Train   0.2562    34.37     41.52     \n",
      "       Val     -         34.37     41.13     \n",
      "Val   Epoch 25  100.00%\n",
      "25     Train   0.3025    33.28     40.75     \n",
      "       Val     -         31.46     38.19     \n",
      "Val   Epoch 26  100.00%\n",
      "26     Train   0.2603    34.63     42.09     \n",
      "       Val     -         32.70     39.66     \n",
      "Val   Epoch 27  100.00%\n",
      "27     Train   0.2821    33.55     40.76     \n",
      "       Val     -         30.80     37.98     \n",
      "Val   Epoch 28  100.00%\n",
      "28     Train   0.2484    33.64     41.03     \n",
      "       Val     -         33.16     40.51     \n",
      "Val   Epoch 29  100.00%\n",
      "29     Train   0.2361    35.43     42.47     \n",
      "       Val     -         33.35     40.27     \n",
      "Val   Epoch 30  100.00%\n",
      "30     Train   0.2223    35.94     43.11     \n",
      "       Val     -         34.24     41.25     \n",
      "Val   Epoch 31  100.00%\n",
      "31     Train   0.2196    36.36     43.56     \n",
      "       Val     -         35.07     42.63     \n",
      "Val   Epoch 32  100.00%\n",
      "32     Train   0.2182    36.72     43.98     \n",
      "       Val     -         35.04     42.20     \n",
      "Val   Epoch 33  100.00%\n",
      "33     Train   0.2160    36.75     43.92     \n",
      "       Val     -         35.06     42.01     \n",
      "Val   Epoch 34  100.00%\n",
      "34     Train   0.2181    37.00     44.26     \n",
      "       Val     -         34.66     41.45     \n",
      "Val   Epoch 35  100.00%\n",
      "35     Train   0.2149    36.93     44.08     \n",
      "       Val     -         35.91     43.01     \n",
      "Val   Epoch 36  100.00%\n",
      "36     Train   0.2098    37.35     44.67     \n",
      "       Val     -         36.22     43.41     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 37  100.00%\n",
      "37     Train   0.2078    37.68     45.04     \n",
      "       Val     -         36.46     43.82     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 38  100.00%\n",
      "38     Train   0.2069    37.62     44.78     \n",
      "       Val     -         35.08     42.15     \n",
      "Val   Epoch 39  100.00%\n",
      "39     Train   0.2016    38.19     45.33     \n",
      "       Val     -         37.10     44.34     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 40  100.00%\n",
      "40     Train   0.2103    38.18     45.33     \n",
      "       Val     -         36.42     43.28     \n",
      "Val   Epoch 41  100.00%\n",
      "41     Train   0.1985    38.12     45.33     \n",
      "       Val     -         36.81     43.92     \n",
      "Val   Epoch 42  100.00%\n",
      "42     Train   0.1967    37.97     45.15     \n",
      "       Val     -         38.09     45.21     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 43  100.00%\n",
      "43     Train   0.1991    38.38     45.52     \n",
      "       Val     -         37.30     44.62     \n",
      "Val   Epoch 44  100.00%\n",
      "44     Train   0.1955    39.62     46.97     \n",
      "       Val     -         38.43     45.88     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 45  100.00%\n",
      "45     Train   0.2217    38.94     46.20     \n",
      "       Val     -         36.50     43.74     \n",
      "Val   Epoch 46  100.00%\n",
      "46     Train   0.2176    37.07     44.79     \n",
      "       Val     -         36.91     43.98     \n",
      "Train Epoch 47  58.75%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      4\u001b[39m log_dir = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msemantic_segmentation/runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \n\u001b[32m      6\u001b[39m trainer = Trainer(\n\u001b[32m      7\u001b[39m     model=model,\n\u001b[32m      8\u001b[39m     train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     max_patience=\u001b[32m15\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m45\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     epoch_loss, epoch_iou, epoch_f1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler.step(epoch_iou)\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Evaluación después de entrenamiento\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    102\u001b[39m loss.backward()\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m batch_iou, f1_score = \u001b[43mcalculate_multiclass_iou_f1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m ious.append(batch_iou)\n\u001b[32m    109\u001b[39m f1.append(f1_score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcalculate_multiclass_iou_f1\u001b[39m\u001b[34m(predicted_logits, target_mask, num_classes, epsilon)\u001b[39m\n\u001b[32m     54\u001b[39m     f1 = (\u001b[32m2\u001b[39m * tp + epsilon) / (\u001b[32m2\u001b[39m * tp + fp + fn + epsilon)\n\u001b[32m     55\u001b[39m     f1s.append(f1)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m mean_iou = torch.mean(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mious\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m mean_f1 = torch.mean(torch.tensor(f1s))\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mean_iou.item(), mean_f1.item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
=======
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
      "1      Train   0.8521    34.84     40.35     \n",
      "       Val     -         43.64     48.57     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Val   Epoch 2  100.00%\n",
      "2      Train   0.6162    39.82     44.99     \n",
      "       Val     -         43.82     48.80     \n",
      "Modelo guardado como TransferSegmentation_Jeff.th\n",
      "Train Epoch 3  63.64%\r"
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "\n",
    "train_logger = None \n",
    "log_dir = f'semantic_segmentation/runs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}' \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    log_dir=log_dir,\n",
    "    num_classes=7,\n",
    "    max_epochs=100,\n",
    "    max_patience=15\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "display_name": "uni",
=======
   "display_name": "Python 3",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "display_name": "Python 3",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "display_name": "Python 3",
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "version": "3.11.13"
=======
   "version": "3.11.12"
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "version": "3.11.12"
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
=======
   "version": "3.11.12"
>>>>>>> f6cb91ebbe6bb75a52033d1f4d3210be2bf53889
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
